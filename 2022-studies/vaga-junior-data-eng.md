# Vagas de Engenheiro de Dados Júnior - 2022


## Vaga 1

Pré-Requisitos Conhecimentos e Experiências
+ Git para controle de versão
+ Construção/manutenção de data warehouse (google bigquery)
+ Conhecimento em infraestrutura cloud para big data
+ Inglês para leitura de documentação técnica;
+ Proficiência em Python e SQL para manipulação de dados;
+ processamento de dados em larga escala com spark (pyspark)
+ Orquestração de dados com Apache Airflow



## Vaga 2


**Responsabilidades e atribuições
**

- Apoiar na definição dos melhores indicadores para o negócio;
- Identificar os dados necessários para composição dos indicadores;
- Codificar a ingestão de dados no Data Lake;
- Codificar a transformação e limpeza dos dados dentro do Data Lake;
- Apoiar na definição de esquemas e catálogo das dados;
- Estruturar os esquemas de dados no Data Warehouse;
- Apoiar a construção de painéis e indicadores para visualização das informações geradas;
- Irá atuar em conjunto com parceiros externos e colegas com os diferentes papéis do processo de dados (analistas de negócio, arquitetos, engenheiros, analistas, etc), seguindo metodologias ágeis em seu dia a dia;
- Difundir e promover o uso das ferramentas de dados e metodologia com os demais colegas da empresa;
- Seguir as práticas de arquitetura e desenvolvimento propostas pela empresa, buscando a eficiência e garantindo um código limpo, seguro e reutilizável;
- Pesquisar, desenvolver-se e sugerir novas práticas e bibliotecas para adoção;
- Manter suas tarefas documentadas e código versionado;

**Requisitos e qualificações
**

- Domínio de SQL;
- Experiência com Python (pandas, numpy, etc);
- Experiência com consumo de APIs (REST, SOAP, etc);
- Experiência com ciclo de desenvolvimento e Git;
- Modelagem de dados;
- Conhecimentos em estatística básica;
- Experiência com Power BI ou afins;
- Vivência com ferramentas AWS (S3, Redshift, Lambda, etc);
- Desejável experiência com dbt;

## Vaga 3

- engenheiros de dados e deverá trabalhar muito próximo dessa equipe.)

O que você precisa para participar:

- Experiência com programação (preferencialmente Python/R/C#), testes unitários e integrados e versionamento de código (git);
- Conhecimento em SQL e banco de dados relacionais, não relacionais e analíticos;
- Conhecer/Ter experiência com ferramentas para processamento de dados e ETL’s (airflow, flink, spark, dask, etc);
- Gostar (bastante) de tecnologia;
- Desejável formação superior completa em áreas de tecnologia (Engenharias, Ciências da Computação, Análise de Sistemas) ou exatas.

Você sairá com uns pontinhos na frente se tiver:

- Conhecimento do ecossistema Azure e Microsoft;
- Conhecimento em shell/bash/unix;
- Experiência com modelagem de bancos para BI - snowflake, star, datavault 2.0;



## Vaga 4

***Segue descrição técnica:\***

- *Projeto voltado para a migração do nosso Lake da Azure para GCP*
- *Atuação com aplicações, arquitetura, linguagem de programação, banco de dados, testes, bpm, BI*
- *Python / SQL*
- *Cloud (Azure / GCP)*
- *Airflow*
- *Diferencial - Ajudar na migração das tabelas da Azure para GCP, bem como reescrever seus processos de carga* *.*

## Vaga 5

Requisitos:
• Necessário: Conhecimento em Python;
• Diferencias: Conhecimento em linguagem spark ou pandas.
• Azure Databricks
• Data Factory
• Synapse Analytics
• Pyspark
• Python
• Linguagem SQL



## Vaga 6

• **Engenheiro de Dados**
Requisitos: Pessoa com conhecimentos em Databricks, Data Factory, SQL, Spark, Conceitos e conhecimentos básicos de arquitetura em Cloud (Azure).





## Vaga 7

- Linguagens/ferramentas: Python, SQL;
- Arquitetura e processos de cargas para DataLake ou DW;
- Extrações de Dados via API´s;
- Construção e manutenção de Pipeline de dados.
- Ferramentas de ETL;
- Ferramentas de orquestração de fluxo de dados;
- AWS (CloudFormation, S3, Lambda, Glue, Redshift, entre outros)
- BigData (DataLake);
- DataOps e esteiras de CI/CD.

## Vaga 8 - Júnior Remoto

Pré-Requisitos Conhecimentos e Experiências
Git para controle de versão
Construção/manutenção de data warehouse (google bigquery)
Conhecimento em infraestrutura cloud para big data
Inglês para leitura de documentação técnica;
Proficiência em Python e SQL para manipulação de dados;
processamento de dados em larga escala com spark (pyspark)
Orquestração de dados com Apache Airflow

## Vaga 9 - JR - https://projetas.com.br/

ENGENHEIRO DE DADOS JR
 Modalidade de contratação: Pessoa Jurídica
 100% Remoto

 Formação/ Especializações/ Certificações:

- Superior Completo nas áreas de TI/Engenharia/Afins
- Certificações AWS (Desejável)

Experiências desejáveis:

- Experiência com bancos de dados não relacionais como: HBase, DynamoDB, Cassandra ou MongoDB;
- Conhecimento de testes unitários
- Vivência básica em processos de carga/gestão de dados em ambiente AWS
- Conhecimento básico de pilha de serviços de Data Lake AWS.

Experiências úteis:

- Experiência com bancos de dados relacionados como: SQL Server, Oracle, PostGreeSQL e MySql;
- Atuação em projetos de BI, Big Data

Perfil Técnico:

- Serviços AWS - EMR          
- PySpark          
- Python    
- Conceitos de Data Lake        
- Linguagem SQL      
- Modelagem de dados      
- Modelagem multidimensional    
- Experiências em ferramentas ETL's        
- Git    
- Serviços AWS - S3    
- Serviços AWS - Redshift      
- Apache Airflow      


Principais atividades:

- Criação dos processos de ingestão em projetos de Data Lake, passando pela camada de encenação e disponibilizando os dados na ferramenta de visualização de dados para o tempo de BI do cliente consumir.  

## Vaga 10 


Algumas De Suas Responsabilidades
Criar grafos de processamento no Airflow;
Desenvolver integrações com APIs e diversos bancos de dados;
Criar e manter as pipelines de Machine Learning;
Desenvolver jobs para processamento em Python;
Criar testes unitários e testes de qualidade de dados;
Contribuir e participar ativamente de comunidades open source (ser colaborativo);
Evoluir os sistemas de monitoramentos e alertas de dados;
Evoluir o sistema marketing direcionado que envia milhões de mensagens por dia.
Requisitos e qualificações

Nosso Match Será Perfeito Se Possuir
Experiência com ao menos um projeto de dados em larga escala
Desenvolvimento de rotinas de processamento e ETL em Python
Desenvolvimento de aplicações em Docker
Desenvolvimento de migrações e consultas em SQL e NoSQL
Diferenciais Para Este Desafio
Conhecimento em Spark
Conhecimento em Clickhouse
Conhecimento em Kafka
Conhecimento em MongoDB
Conhecimento em Airflow
Conhecimento em Kubernetes
Conhecimento em modelos de machine learning.
Informações adicionais


